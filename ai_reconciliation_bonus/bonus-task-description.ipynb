{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b52e8afa",
   "metadata": {},
   "source": [
    "\n",
    "# AI Reconciliation Bonus Report\n",
    "\n",
    "## Objective\n",
    "\n",
    "The task was to reconcile a schema-mismatched customer dataset (reconciliation_challenge_data.csv) with an existing cleaned schema (orders_clean.csv / v1_ecommerce.db::customers). The goal was to use AI tools to assist in schema alignment and demonstrate analytical thinking and creative problem-solving.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Initial Dataset Understanding\n",
    "\n",
    "Two datasets were loaded:\n",
    "\n",
    "- reconciliation_challenge_data.csv: A noisy, mismatched schema with transactional + customer-level attributes\n",
    "- orders_clean.csv / orders table: Our reference schema with standardized order fields\n",
    "\n",
    "Initial inspection showed that many column names in the new dataset did not directly align with the expected schema, and some columns had ambiguous meanings (e.g. payment_status, delivery_status).\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Fuzzy Mapping Using Jaccard Similarity\n",
    "\n",
    "To begin the reconciliation, I implemented a string-based fuzzy mapping approach using a Jaccard similarity function defined as:\n",
    "\n",
    "```python\n",
    "def jaccard_similarity(a, b):\n",
    "    a_set = set(a.lower().split(\"_\"))\n",
    "    b_set = set(b.lower().split(\"_\"))\n",
    "    return len(a_set & b_set) / len(a_set | b_set)\n",
    "```\n",
    "\n",
    "This was applied to every pair of (new_column, reference_column) to build a similarity matrix. While it helped shortlist candidates (payment_status -> payment_method, customer_segment -> possibly inferred from order behavior), many mappings were weak or misleading due to limited semantic overlap in names.\n",
    "\n",
    "Conclusion: Jaccard was not sufficient alone for semantic understanding.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: AI-Assisted Reconciliation with Gemini\n",
    "\n",
    "To improve the mapping quality, I integrated Gemini AI (model: gemini-1.5-flash-latest) using the google.generativeai Python SDK.\n",
    "\n",
    "For each new column:\n",
    "- I sampled a few representative values from the dataset\n",
    "- Constructed a prompt: \"Given these values and the reference schema, which field in the reference schema is semantically equivalent to this column?\"\n",
    "\n",
    "This approach significantly improved match precision, especially for semantically aligned fields like:\n",
    "- full_customer_name â†’ full_name\n",
    "- etc\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Final Column Mapping Strategy\n",
    "\n",
    "The final schema reconciliation was constructed as a union of:\n",
    "\n",
    "- AI-suggested mappings (via Gemini)\n",
    "- Human judgment from fuzzy matches\n",
    "- Manual logic for ambiguous columns \n",
    "\n",
    "Columns that did not map to the reference schema (e.g. item_reference, shipping_fee) were retained for completeness but not integrated.\n",
    "\n",
    "Final joining was done with my corrected mappings.\n",
    "---\n",
    "\n",
    "## Step 5: Data Quality Checks & Visualization\n",
    "\n",
    "Post-reconciliation, I performed a series of sanity checks:\n",
    "\n",
    "- No missing critical fields (full_name, segment, status)\n",
    "- Valid date formats and timestamps\n",
    "- Outlier detection in numeric fields (total_spent, amount_paid)\n",
    "- Visual distribution of status and segment values confirmed balanced dataset\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Row-Level Integration (Planned)\n",
    "\n",
    "Schema alignment was successfully achieved. The next step is row-level integration: matching reconciled records to the existing orders table (using fields like cust_id, order_total, order_datetime) and enriching/updating order records.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This reconciliation task involved:\n",
    "- Combining rule-based fuzzy logic with LLM-driven semantic alignment\n",
    "- Parsing and verifying structured AI responses\n",
    "- Applying domain reasoning to finalize mappings\n",
    "\n",
    "The outcome was a fully aligned dataset with explainable, traceable decisions across the pipeline. \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
